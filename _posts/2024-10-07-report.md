---
title: 주간 보고서(9.30~10.06)
tags: TeXt
---

---

## 업무요약

<aside>
<img src="https://www.notion.so/icons/light-bulb_yellow.svg" alt="https://www.notion.so/icons/light-bulb_yellow.svg" width="40px" />
</aside>

- 박종현 : 주가 데이터 분석 및 종목 추출 작업(노가다)
- 이제욱 :  Terraform을 이용한 aws 프로비저닝, fastapi test code docker로 말아 ec2 배포 및 통신 테스트, github action으로 ci/cd 구축
- 조민수 :  api연결, 주가 데이터 추출, 볼린저밴드 계산식작성
- 조하은 :  뉴스 종목별 크롤링


---


# 세부 업무 내용

## **Terraform을 이용한 인프라 프로비저닝**

- **local에서 aws cli을 통해 접근하기 위한 iam-user 생성 및 aws 연동하기 위한 access key 생성**

![image.png](/assets/images/report/_5_report/image.png)

- **Terraform 프로비저닝 자동화**

![image.png](/assets/images/report/_5_report/image%201.png)

vpc와 ec2 따로 모듈화해서 수정 편이하게끔 코드 작성. 추후 rds 필요할 경우 코드 수정해서 즉각 반영 가능. command 한 줄 입력만으로 즉각 모든 인프라 삭제하여 비용을 절약할 수 있다. 추후 rds 사용할 시 private subnet과 db 인터넷 연결 필요할 시 nat gateway 생성. 현재는 비용 문제로 bastion 탄력적 ip 생성하지 않은 상태. 실 서비스 시작 시 탄력적 ip 생성 예정.

- **terraform apply 생성 결과**

![image.png](/assets/images/report/_5_report/image%202.png)

- **ec2 생성 및 vpc 생성**

![image.png](/assets/images/report/_5_report/image%203.png)

- **terraform destroy로 cli로 인프라 삭제**

![image.png](/assets/images/report/_5_report/image%204.png)

삭제된 것 확인 가능하다. 

- **ec2 ssh 연결. local에서 ssh 접근 및 외부 인터넷 연결 확인**

![image.png](/assets/images/report/_5_report/image%205.png)

## Fast api, docker test

- **iris dataset으로 간단한 모델 완성**

```python
import joblib
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier

# Load the iris dataset
iris = load_iris()
X, y = iris.data, iris.target

# Train a random forest classifier
model = RandomForestClassifier()
model.fit(X, y)

# Save the trained model
joblib.dump(model, 'model.joblib')
```

- **fastapi 코드 작성 /predict endpoint로 curl을 보내 test**

```python
from fastapi import FastAPI
import joblib
import numpy as np

app = FastAPI()

# Load the trained model
model = joblib.load('model.joblib')

@app.get("/")
def read_root():
    return {"message": "Welcome to the ML Model API V1"}

@app.post("/predict/")
def predict(data: dict):
    features = np.array(data['features']).reshape(1, -1)
    prediction = model.predict(features)
    class_name = iris.target_names[prediction][0]
    return {"class": class_name}
```

- **docker hub에 이미지 빌드 후 업로드**

![image.png](/assets/images/report/_5_report/image%206.png)

- local에서 test

![image.png](/assets/images/report/_5_report/image%207.png)

- / endpoint로 접근

![image.png](/assets/images/report/_5_report/image%208.png)

- 모델 호출을 위해 /predict로 접근

![image.png](/assets/images/report/_5_report/image%209.png)

iris dataset중 setosa에 해당한다는 결과값을 받을 수 있었다.

- **Ec2 fastapi 컨테이너 실행 후 연결 테스트**

보안그룹에서 8000 port tcp 연결 추가 → docker image pull 후 실행 → 브라우저에서 ip 단위로 접근 → 모델 호출

![image.png](/assets/images/report/_5_report/image%2010.png)

마찬가지로 iris dataset중 setosa에 해당한다는 결과값을 받을 수 있었다.

### CI / CD 적용

- github repo에 미리 환경변수 설정 후 .github/workflows 하위 디렉토리로 yml 파일 push

![image.png](/assets/images/report/_5_report/image%2011.png)

- **CI**

```python
name: Build to dockerhub

on:
  push:
    branches:
      - main  # exec when pushed main branch

jobs:
  build:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Build Docker image
        run: |
          docker build -t ${{ secrets.DOCKER_USERNAME }}/fastapi_test:v1 .

      - name: Login to Docker Hub
        run: echo "${{ secrets.DOCKER_TOKEN }}" | docker login -u "${{ secrets.DOCKER_USERNAME }}" --password-stdin

      - name: Push Docker image to Docker Hub
        run: |
          docker push ${{ secrets.DOCKER_USERNAME }}/fastapi_test:v1

```

- **CD**

```python
  deploy:
    needs: build
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to EC2
        run: |
          echo "${{ secrets.EC2_KEY }}" > ec2_key.pem
          chmod 600 ec2_key.pem
          ssh -o StrictHostKeyChecking=no -i ec2_key.pem ec2-user@13.124.230.180 << 'EOF'
            docker pull ${{ secrets.DOCKER_USERNAME }}/fastapi_test:v1
            docker stop my_fastapi_container || true
            docker rm my_fastapi_container || true
            docker run -d -p 8000:8000 --name my_fastapi_container ${{ secrets.DOCKER_USERNAME }}/fastapi_test:v1
          EOF
```

- **무중단배포 확인**

![image.png](/assets/images/report/_5_report/image%2012.png)

ec2에서 확인 시 컨테이너가 재생성된 것 확인 및 버전 확인. V1에서 V2로 바뀐 것을 확인 가능하다. 

- **WORKFLOW**

![image.png](/assets/images/report/_5_report/image%2013.png)

## 상장 종목 추출 및 기사 크롤링

### 1. 상장 법인 목록 추출

```python
import pandas as pd
import FinanceDataReader as fdr

def load_allstock_KRX():
    krx_url = 'https://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13'
    stk_data = pd.read_html(krx_url, header=0)[0]
    stk_data = stk_data[['회사명', '종목코드']]
    stk_data = stk_data.rename(columns={'회사명': 'Name', '종목코드': 'Code'})

    stk_data['Code'] = stk_data['Code'].apply(lambda input: '0' * (6 - len(str(input))) + str(input))

    return stk_data
```

한국 거래소에 상장된 종목명과 종목 코드를 추출해 왔다. API를 사용하면 종목 코드를 바로 추출할 수 있지만, 뉴스 크롤링이 잘 되는지 확인하기 위해 법인 목록을 추출하였다. 

[상장법인목록.xls](/assets/images/report/_5_report/%25EC%2583%2581%25EC%259E%25A5%25EB%25B2%2595%25EC%259D%25B8%25EB%25AA%25A9%25EB%25A1%259D.xls)

### 2. 법인 목록 xls에서 종목명/종목 코드만 추출

```python
import openpyxl

def read_excel(file_name, columns):

    # 엑셀 파일 열기
    workbook = openpyxl.load_workbook(file_name)
    
    # 첫 번째 시트를 선택
    sheet = workbook.active
    
    # 시트의 데이터를 추출하고 출력
    for row in sheet.iter_rows(min_col=1, max_col=sheet.max_column, values_only=True):
        selected_data = [row[col-1] for col in columns]
        print(selected_data)

# 엑셀 파일 경로
file_name = '상장법인목록.xlsx'
columns = [1, 2]
read_excel(file_name, columns)
```

로컬 파일에서 종목 이름/코드를 불러오기 위해 이름과 코드만 따로 추출하여 json파일에 저장하였다.

### 3. 기사 크롤링

```python
from bs4 import BeautifulSoup
import requests
import re
import pandas as pd

page = 1 #페이지는 200까지 제공됨
company_code = "005930" # 종목 코드 -> 이 코드는 삼성전자임

# 선택된 종목 크롤링하기 위한 url 
url = 'https://finance.naver.com/item/news_news.nhn?code=' + str(company_code) + '&page=' + str(page) 

# 해당 페이지의 html 파일들 가지고 오기
source_code = requests.get(url).text
#html = BeautifulSoup(source_code, "lxml")
html = BeautifulSoup(source_code,"html.parser")

# 뉴스 제목가지고 오기 (class = title로 되어 있음), text만 가지고 오고 엔터문자는 제외
titles = html.select('.title')
title_result=[]

for title in titles: 

    title = title.get_text() 
    title = re.sub('\n','',title)
    title_result.append(title)

# 링크 가지고 오기, title 클래스에 a 밑에 herf되어 있는 것 가지고 오기
links = html.select('.title') 

link_result =[]

for link in links: 

    add = 'https://finance.naver.com' + link.find('a')['href']

#    add = link.find('a')('href')

    link_result.append(add)

# 뉴스 날짜가지고 오기
dates = html.select('.date') 
date_result = [date.get_text() for date in dates] 

# 뉴스 매체 가지고 오기
sources = html.select('.info')
source_result = [source.get_text() for source in sources] 

articles = []

for article in link_result:

    headers = {"user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 11_1_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36"}

    response = requests.post(article, headers=headers)
    dom = BeautifulSoup(response.content, "html.parser")
    # articles.append(dom.select_one("div#news_read").get_text())

    redirect_url = dom.find('script').string
    redirect_url = redirect_url[:-2].split("href='")[1]

    print(redirect_url);

    final_response = requests.get(redirect_url, headers=headers)
    final_soup = BeautifulSoup(final_response.text, "html.parser")

    title = final_soup.select_one('h2.media_end_head_headline span').getText();
    date = final_soup.select_one('div.media_end_head_info_datestamp_bunch span').getText();
    #source = final_soup.select_one('img.media_end_head_top_logo_img light_type._LAZY_LOADING._LAZY_LOADING_INIT_HIDE');
    img_tag = final_soup.find('img', class_='media_end_head_top_logo_img')
    source = img_tag['alt'] if img_tag else "No alt attribute found"
    article_content = final_soup.select_one('div#newsct_article').get_text()

    #print(source);

    # source = final_soup.select_one('em.media_end_head_journalist_name').get_text()
    # article_content = final_soup.select_one('div#newsct_article').get_text()

    articles.append({"날짜" : date, "언론사" : source, "기사제목" : title, "링크" : redirect_url, "기사원문": article_content})   
```

종목 뉴스 url은 종목 코드와 페이지를 포함한다. 1페이지부터 200페이지까지 존재하며 한 페이지 당 10개의 뉴스, 총 2000개의 뉴스를 종목별로 제공하고 있다. 위의 코드는 삼성 전자 관련 뉴스중 1페이지에 존재하는 최신 기사 10개를 크롤링하는 코드이다. 개발자 도구를 활용하여 각 컴포넌트의 클래스들을 확인한 뒤 BeautifulSoup을 사용하여 추출하였다.

날짜를 확인하여 작일/금일의 뉴스를 크롤링하는 코드를 추가할 예정이다.

---

## 종목 추출 및 데이터 분석

![image.png](/assets/images/report/_5_report/image%2014.png)

 키움 증권의 “성과검증(조건검색)”을 기능을 이용하여 과거 특정 시점에 검색기에 검출된 종목들을 수집 중이다. 총 4개의 검색기로 9시부터 분단위로 일일히 검색을 하고, 검출된 종목 이름을 옮겨 적어야 하기 때문에 상당히 많은 시간이 소요되고 있다. 과거 6개월 시점부터 수집을 진행하고 있으며, 하루 당 약 10개 이상의 종목들이 수집되고 있다. 과거 종목 리스트가 추출이 완료되면, 조민수 학생에게 넘겨 해당 날짜에 기록된 주가 데이터를 대신증권 API로 받아오고, 이외에 기술지표 및 입력값들을 형성할 것이다.

 또한 모델 정확성 향상을 위하여, 주가 반등에 가장 중요한 요인들을 재검토하고, 이들을 학습데이터로 구성하기 위한 방안을 모색하는 중이다. 일반적으로 시계열 예측, 분류 모델에는 Sliding Window 기법을 적용하여 학습데이터를 구성하지만, 값이 일정기간 고정되어 있는 특성값도 존재하고, 무엇보다 움직이는 Window 밖에 존재할 수 있는 값(Ex. 당일 최고가)을 포함시켜야 한다. 종목 추출과 데이터 분석을 병행하며 최종적으로 입력에 필요한 모든 지표들을 결정할 것이다.

---

### 1.Api연결

- 대신증권 api 접속 - clear
    
    ![cybos.PNG](/assets/images/report/_5_report/cybos.png)
    

→ 다음과 같이 대신증권 api인 CybosPlus를 통해서 api 접근이 가능하다

해당 api의 경우 win32만을 지원하고 있어 다음과 같이 win32com.client를 사용한다

```python
import win32com.client
```

다음으로 api연결을 하기위해다음과 같은 코드를 사용하였다.

![연결.PNG](/assets/images/report/_5_report/%25EC%2597%25B0%25EA%25B2%25B0.png)

### 2.주식 데이터 추출

- Api 과거 주식 데이터 추출 - 2년 단위 추출 가능
- 과거시점에 대한 조건검색 결과가 최대 6개월 까지 가능 → 우선 3개월 단위 과거 데이터 추출 완료.
    - 종목코드.csv 파일로 저장 완료(ex. A005930.csv)
    - 종목코드, 날짜, 시간, 시가, 고가, 저가, 종가, 거래량, 거래대금, 누적체결매수수량, 누적체결매도수량 데이터

### 2-1) 날짜 검색

- 과거 주식 데이터들 중 특정 과거 시점에 대한 데이터 추출 - clear
    - 데이터 조회시 날짜를 설정하면 완료.
    
    ```python
    chart = win32com.client.Dispatch("CpSysDib.StockChart")
    
        chart.SetInputValue(0, stock_code)  # 종목 코드
        chart.SetInputValue(1, ord('1'))    # 1: 기간으로 설정
        chart.SetInputValue(2, end_date)    # 요청 끝 날짜
        chart.SetInputValue(3, start_date)  # 요청 시작 날짜
        chart.SetInputValue(5, [0, 2, 3, 4, 5])  # 0: 날짜, 2: 시가, 3: 고가, 4: 저가, 5: 종가
        chart.SetInputValue(6, ord('m'))    # 'm': 분봉 데이터 요청
        chart.SetInputValue(9, ord('1'))    # 수정주가 사용 여부 (1: 사용)
    ```
    

### 2-2) 조건 검색

- 조건 검색을 사용하여 특정 주식 추출 - clear
    
    ![종목검색.PNG](/assets/images/report/_5_report/%25EC%25A2%2585%25EB%25AA%25A9%25EA%25B2%2580%25EC%2583%2589.png)
    
    →종목코드, 종목 이름 추출, 추가적으로 실시간 감시를 통해 결과에 변화가 있으면 실시간 수정.
    

### 3.볼린저밴드

대신증권 api의 경우 기본적으로 볼린저 밴드값을 제공 X→ 필요시 직접 계산 필요.

과거 주식데이터를 기반으로 볼린져 밴드 계산식

```python
#과거 주식 데이터 추출
def get_minute_data(stock_code, start_date, end_date):
    chart = win32com.client.Dispatch("CpSysDib.StockChart")

    chart.SetInputValue(0, stock_code)  # 종목 코드
    chart.SetInputValue(1, ord('1'))    # 1: 기간으로 설정
    chart.SetInputValue(2, end_date)    # 요청 끝 날짜
    chart.SetInputValue(3, start_date)  # 요청 시작 날짜
    chart.SetInputValue(5, [0, 2, 3, 4, 5])  # 0: 날짜, 2: 시가, 3: 고가, 4: 저가, 5: 종가
    chart.SetInputValue(6, ord('m'))    # 'm': 분봉 데이터 요청
    chart.SetInputValue(9, ord('1'))    # 수정주가 사용 여부 (1: 사용)

    chart.BlockRequest()

    num_data = chart.GetHeaderValue(3)  # 데이터 개수 확인

    data = []
    for i in range(num_data):
        date = chart.GetDataValue(0, i)
        open_price = chart.GetDataValue(1, i)
        high_price = chart.GetDataValue(2, i)
        low_price = chart.GetDataValue(3, i)
        close_price = chart.GetDataValue(4, i)
        data.append([date, open_price, high_price, low_price, close_price])

    # Pandas DataFrame으로 변환
    columns = ['Date', 'Open', 'High', 'Low', 'Close']
    df = pd.DataFrame(data, columns=columns)

    return df
#과거 데이터를 기반으로 볼린저밴드 계산
def calculate_bollinger_bands(df, window=20, num_std=2):
    # 종가 기준으로 이동평균(SMA) 계산
    df['SMA'] = df['Close'].rolling(window=window).mean()

    # 종가 기준으로 표준편차 계산
    df['STD'] = df['Close'].rolling(window=window).std()

    # 볼린저 밴드 상단, 하단 계산
    df['Upper Band'] = df['SMA'] + (df['STD'] * num_std)
    df['Lower Band'] = df['SMA'] - (df['STD'] * num_std)

    return df
```

종목 조회하면서 나오는 데이터(SMA)를 기반으로 볼린저밴드값 계산.



---

If you like TeXt, don't forget to give me a star. :star2:

[![Star This Project](https://img.shields.io/github/stars/kitian616/jekyll-TeXt-theme.svg?label=Stars&style=social)](https://github.com/kitian616/jekyll-TeXt-theme/)
